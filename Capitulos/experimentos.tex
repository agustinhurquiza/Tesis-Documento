\chapter{Experimentos}\label{cap:experimentos}

En este capitulo explican los distintos experimentos desarrollados, se informar sobre los valores obtenidos en este proyecto y se compara con otros trabajos. 


\section{Experimentos previos}

\subsection{Experimentación con propuesta de objetos} \label{ssec:experimentacionconpropuestadeobjetos}
Como se mencionó anteriormente, el número de propuestas es un parámetro clave. Algunas métricas son muy sensibles a la cantidad de propuestas, afectando así los resultados finales. Esto se observó cuando se obtuvieron las primeras métricas, donde los valores estaban muy lejos de los esperados, y a medida que se aumentaba la cantidad de propuestas, los resultados empeoraban. Por este motivo, se probaron dos algoritmos, \textit{Edge Boxes}~\cite{zitnick2014edge} y \textit{Selective Search}~\cite{uijlings2013selective}, con algunas combinaciones de sus parámetros, con el objetivo de obtener una cantidad de propuestas que se superponga con el mayor número de objetos sin afectar las métricas.\\

Para no sesgar el experimento con los datos de prueba, se definió la metodología de la siguiente manera: por cada imagen de entrenamiento se corrió el generador de propuestas, y se calculó el tiempo y la cantidad de cuadros verdaderos que tenían un IoU $> 0.5$, con algún cuadro verdadero. En nuestros experimentos, el tiempo es un parámetro importante ya que algunos algoritmos soy muy lentos y resultan difíciles de evaluar con los recursos disponibles. 

Como se puede observar en la \autoref{tabla:edgeVSselct}, \textit{Selective Search} obtiene una mayor cantidad de superposición, pero con un número exageradamente grande de propuestas, por lo que resulta mas ventajoso usar \textit{Edge-boxes}~\cite{zitnick2014edge}. En cuanto número de propuestas totales, resulta más conveniente entre 100 y 500 propuestas como máximo, ya que al aumentar este numero no se generan mejoras en superposición. Si tenemos en cuenta el tiempo, resulta mas practico \textit{Edge-boxes}~\cite{zitnick2014edge}, ya que demora una fracción de lo que tarda \textit{Selective Search}~\cite{uijlings2013selective}. En la \autoref{fig:boxes_cuantitativos} se pueden observar las propuestas de cuadros delimitadores para ambos algoritmos.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!} {
		\begin{tabular}{|c|c|c|r|r|}
			\hline
			\multicolumn{2}{|c|}{Algoritmo}                   & Numero de propuestas & \multicolumn{1}{c|}{Tiempo promedio(S)} & \multicolumn{1}{c|}{proporción(*)} \\ \hline
			\multicolumn{2}{|c|}{\multirow{4}{*}{Edge Boxes}} & 100                  & 0.11                                    & 0.01953                                                \\ \cline{3-5} 
			\multicolumn{2}{|c|}{}                            & 500                  & 0.11                                    & 0.00607                                                \\ \cline{3-5} 
			\multicolumn{2}{|c|}{}                            & 1000                 & 0.12                                    & 0.00355                                                \\ \cline{3-5} 
			\multicolumn{2}{|c|}{}                            & 5000                 & 0.12                                    & 0.00120                                                \\ \hline
			\multirow{2}{*}{Selective Search}     & Single    & $\approx$ 5000       & 5.40                                    & 0.00063                                                \\ \cline{2-5} 
			& Fast      & $\approx$ 1000       & 1.41                                    & 0.00212                                                \\ \hline
		\end{tabular}
	}                                                 
	\caption{Resultados de correr los distintos algoritmos de propuestas de regiones en los datos de entrenamiento. La última columna muestra el promedio para todas las imágenes de la relación: (*)$(Propuestas\ con\ IOU > 0.5) / numero\ de\ Propuetas$.}
	\label{tabla:edgeVSselct}
\end{table}

\begin{figure}
	\begin{center}
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{original_box.jpg}
			\caption{Imagen Original}
			\label{fig:sfig1}
		\end{subfigure}\\
	\end{center}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{edge_box.png}
		\caption{Edge Boxes~\cite{zitnick2014edge}}
		\label{fig:sfig2}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{slective_box.png}
		\caption{Selective Search~\cite{uijlings2013selective}}
		\label{fig:sfig3}
	\end{subfigure}
	\begin{center}
	\end{center}
	\caption{Se muestran los resultados obtenidos para los algoritmos Edge Boxes~\cite{zitnick2014edge} con 500 propuestas, y Selective Search~\cite{uijlings2013selective} (Single $\approx$ 5000 propuestas). Ademas, se muestra la imagen original para referencia.}
	\label{fig:boxes_cuantitativos}
\end{figure}


\subsection{Experimentación con CNN} \label{ssec:experimentacionconcnn}
% para generar tablas de latex
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{img/vgg-vs-resnet}
	\caption{Frecuencia de la similitud coseno de los vectores de caracteristicas visuales ente caballo vs caballo, caballo vs camión y caballo vs perro, para las CNN Inception ResNet V2~\cite{resnet} y VGG16~\cite{simonyan2014very}. Los histogramas están normalizados para una mejor visualización de la superposición.}
	\label{fig:vgg-vs-resnet}
\end{figure}

Se decidió analizar la CNN ya que el modelo final es muy dependiente de esta red y de su capacidad de extraer características visuales útiles. Lo que se quiere aquí es que la red sea capaz de asociar las características visuales de objetos similares, y diferenciar los elementos de distinta naturaleza. En otras palabras, el espacio resultante tiene que distribuirse de tal manera que, por ejemplo, las imágenes de los animales estén muy cerca y a su vez alejadas de vehículos o electrodomésticos, pero también mantengan una separación entre los distintos animales como perro y gato. Bansal \etal~\cite{bansal2018zero} propone utilizar Inception ResNet V2~\cite{resnet}, el problema de esta red es que puede resultar pesada en cuanto a tiempo de ejecución y memoria. Por este motivo se decidió intentar con VGG16~\cite{simonyan2014very}, que reduce el número de parámetros en las capas convolucionales y mejorar el tiempo de ejecución, además, es una de la más utilizadas.\\

El experimento consistió en comparar miles de recuadros de 3 clases de entrenamiento, caballo, perro y camión.  Por cada cuadro se generó el vector de caracteristicas visuales. Luego se comparó utilizando la similitud coseno, entre todas las características de caballo vs caballo, caballo vs camión y caballo vs perro. En la \autoref{fig:vgg-vs-resnet} se graficaron las frecuencias de los resultados para cada CNN. Con esto se intenta observar como se distribuyen en el espacio visual las distintas clases. Como se esperaba, la similitud entre entre animales es más grande que con un vehículo. Se observó que para Inception ResNet V2~\cite{resnet} existe una mayor separación entre clases, aunque sus similitudes están más dispersas. VGG16~\cite{simonyan2014very} parece tener una menor dispersión, pero la similitud coseno entre distintas clases tiene valores muy cercanos. Esto puede afectar de manera negativa ya que camión y caballo no poseen una gran diferencia y el modelo podría interpretarlo como clases similares.\\


\section{Detalles de metodología de evaluación} \label{sec:detallesdemetodologiadeevaluacion}
Como ya se menciono, el trabajo de Bansal \etal~\cite{bansal2018zero} carece de una implementación publica. Por este motivo el principal experimento consistió en replicar los resultados de este trabajo, la falta de una implementación nos permitió experimentar con las distintas etapas y parámetros del modelo. Además, decidimos enfocarnos en los modelos que presentaban un mejor desempeño en el trabajo de Bansal \etal~\cite{bansal2018zero}. Por ejemplo, los experimentos con clases de fondo, no obtuvieron buenos resultados, en comparación con los que no la utilizan, por este motivo no lo replicamos.\\

Ahora definamos la metodología de evaluación. El primer paso consiste generar propuestas para cada imagen, luego, cada cuadro propuesto es reescalado al tamaño de la capa de entrada que tiene la CNN, y se le extrae el vector de características visuales. Después, se utiliza el modelo entrenado para inferir el vector de características semánticas, y se calcula la similitud coseno con los vectores semánticos de todas las clases o solo las invisibles, dependiendo si se quiere evaluar GZSD o ZSD. Aquella clase que obtenga el mayor puntaje es asignada a la propuesta, y también se guarda la similitud coseno como la confianza de predicción, que indica que tan ``correcta'' es la predicción. Por último, se agrupan todas las propuestas que se tengan asignada la misma clase y se corre un algoritmo de supresión no máxima. Éste elimina las predicciones repetidas y retorna las mejores propuestas de cada grupo. Al final obtenemos como resultado un conjunto de propuestas, sus clases y su respectivo puntaje. Con estos datos alimentamos la implementación de Padilla \etal~\cite{padilla2020survey}, para obtener los resultados de las métricas.

\section{Resultados cuantitativos} \label{sec:resultadoscuantitativos}

Esta sección desarrolla de forma numérica los resultados obtenidos por los distintos modelos y en las distintas métricas. Se analizan las configuraciones ZSD Y GZSD.

\subsection{Resultados ZSD}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!} {
		\begin{tabular}{|l|c|c|c|c|c|}
			\hline
    		\multicolumn{3}{|l|}{}                                                   & \multicolumn{2}{l|}{Modelo propuesto}               &                               \\ \hline
			\multicolumn{1}{|c|}{{Métrica}} & {Baseline \cite{bansal2018zero}} & \multicolumn{1}{l|}{{DSES \cite{bansal2018zero}}} & {VGG} & { ResNet} & \multicolumn{1}{l|}{{Mejor resultado de \cite{rahman2020zero}}} \\ \hline
			100@Recall (Bansal)             & 22.14                            & 27.19                                             & 26.34              & \textbf{28.91}                 & -                                                               \\ \hline
			100@Recall                      & -                                & -                                                 & 5.44               & 6.38                  & \textbf{12.27}                                                           \\ \hline
			mAP@0.5                         & 0.32                             & 0.54                                              & 0.19               & 0.23                  & \textbf{5.05}                                                            \\ \hline
			mAP@[.5, .95]                   & -                                & -                                                 & 0.17               & \textbf{0.21}                  & -                                                               \\ \hline
		\end{tabular}
	}
	\caption{Comparación de los resultados obtenidos en el presente trabajo, con los obtenidos en Bansal \etal~\cite{bansal2018zero} y Rahman \etal~\cite{rahman2020zero}. Se presentan las distintas métricas \textit{recall} y \textit{mAP}, evaluados en COCO.}
	\label{tab:resultadosZSD}
\end{table}


La \autoref{tab:resultadosZSD}, muestra los valores de las métricas \textit{100@Recall}, en la versión desarrollada por Bansal \etal~\cite{bansal2018zero}, la de Padilla \etal~\cite{padilla2020survey}, \textit{mAP@0.5} y \textit{mAP[.5, .95]}, para los modelos de Bansal \etal~\cite{bansal2018zero}, y los dos propuestos por nosotros, uno utilizando \textit{VGG16} y el otro Inception ResNet V2. Además, se agregan los mejores resultado presentado por el trabajo de Rahman \etal~\cite{rahman2020zero}. Se eligió este documento ya que es un trabajo más actual y aborda de una manera similar a la nuestra el problema de ZSD, aunque presenta algunas mejoras y un modelo más complejo.\\

La \textit{100@Recall} es un buen punto de partida para analizar el modelo propuesto ya que refleja el numero de propuestas predichas correctamente sobre el total de cuadros verdaderos. Obtuvimos 6.38 puntos en esta métrica, que resulta por debajo de lo esperado. Pero esto no significa necesariamente que el modelo no funciona correctamente, existen varios parámetros que influyen en este resultado. El punto que más afecta es el generador de propuestas, ya que menos del 50\% de los cuadros verdaderos obtienen una propuesta con $IoU > 0.5$ lo cual reduce mucho la esperanza de esta métrica. Otro punto, es la capacidad de la CNN de obtener un espacio de características visuales, que agrupe las clases visualmente similares y separe las diferentes. Como se vio en la \autoref{ssec:experimentacionconcnn}, \textit{ResNet} supera a \textit{VGG16} en esta tarea, lo cual se refleja en la pequeña mejora del modelo que utiliza  \textit{ResNet}.

En cuanto a \textit{mAP} obtuvimos 0.23 puntos, esto es un bajo desempeño comparado con los trabajos publicados recientemente, pero al igual que \textit{100@Recall} se ve afectada por los puntos antes mencionados. Además, existe otro factor que la afecta muy fuertemente, debido a la naturaleza de la matriz que proyecta las características visuales al espacio semántico, que hace que dos objetos proyectados obtengan una similitud coseno poco distanciada, la cual ronda entre los valores 0.3 y 0.6. Es decir, si se proyectan dos imágenes con muy pocas diferencias se obtendrá un similitud como máximo de 0.6. Esta similitud es utilizada como el puntaje de confianza de una predicción, y como se explicó en la \autoref{ssec:definiciondemetricas}, para calcular la curva AUC, \textit{mAP} varia el límite de confianza y lo compara con los puntajes obtenidos por cada predicción. Esto hace que cuando el límite sea superior a 0.6 se obtengan valores muy bajos o incluso nulos de \textit{precisión}.\\


El mejor resultado de Bansal \etal~\cite{bansal2018zero}, el cual se denomina \textit{Densely Sampled Embedding Space (DSES)}, consiste en aumentar los datos de entrenamiento con datos adicionales de fuentes externas y obtiene 27.19 puntos en su definición de \textit{recall}. Nuestro modelo base, que usa \textit{ResNet}, supera ese valor obteniendo 28.91 puntos. Esto se debe a que en la etapa de depuración se modificaron algunos parámetros por defecto del entrenamiento, como el numero de lote, la tasa de aprendizaje, el optimizador, etc.  

En cuanto mAP, Bansal\etal~\cite{bansal2018zero} no aporta mucha información y su implementación es desconocida, por lo cual asumimos que lo reportado es \textit{mAP@0.5}. De inmediato se puede observar que los valores son muy bajos 0.54. Esto genera una discrepancia con su alto rendimiento de \textit{100@Recall} y refleja lo poco representativa de esta última métrica. 

Si comparamos con un trabajo más actual~\cite{rahman2020zero} que obtiene 12.27 en \textit{100@Recall} y un excelente desempeño en \textit{mAP@0.5} con 5.05 puntos, refleja una consistencia con nuestros valores y demuestra que la implementación utilizada para calcular las métricas está mejor encaminada.\\


Los resultados de la \autoref{tab:resultadosZSD} se calcularon utilizando la división de clases propuesta por Bansal, ya que ambos trabajos con los que comparamos utilizan esta división. También se corrieron algunas pruebas con la partición propuesta en este trabajo (\autoref{ssec:commonobjectsincontext}), y se observó que los resultados se vieron afectados entre un 4\% y 7\% menos al utilizar nuestra división. Esta reducción se debe a que el documento de Bansal \etal~\cite{bansal2018zero} utiliza como criterio de división los vectores semánticos de las clases, lo cual afecta positivamente ya que es el mismo espacio utilizado para asociar e inferir las clases invisibles.\\

También se calcularon las métricas para el conjunto de datos CIFAR-ZSD, sin embargo, fueron necesarias algunas mejoras para adaptar las diferencias con COCO. Se utilizó una tamaño de entrada para la CNN más pequeña de 32x32, ya que las imágenes no tenían una gran resolución. También, se redujo considerablemente el número máximo de propuestas, de 500 al orden de 50. La justificación de esto es que los objetos sobresalen del fondo de la imagen y es más fácil su detección.

Dicho esto, los resultados obtenidos fueron, 8.83 en \textit{100@Recall} (implementación de Padilla \etal~\cite{padilla2020survey}) y 0.72 para \textit{mAP@0.5}. Estos valores, al contrario de los reportados para COCO, se ven influenciado por la calidad de la imagen, lo que hace muy difícil de diferenciar el aspecto visual de las distintas clases.\\

En conclusión, sabiendo que nuestro modelo de base es muy sencillo y no utiliza ningún tipo de información extra, los valores obtenidos son aceptables para \textit{100@Recall}, aunque resulta importante resaltar su bajo desempeño en \textit{mAP}. 

\subsection{Resultados GZSD}
Por último, se analizaron los resultados en el desafio de GZSD. La configuración generalizada de aprendizaje sin ejemplos es más realista que la configuración de aprendizaje sin ejemplos discutida anteriormente, porque tanto las clases visibles como las invisibles están presentes durante la evaluación.

La metodología de evaluación es la misma que ZSD estándar solo que ahora se agrega las clases visibles a las pruebas. Algunos trabajos modifican la metodología de evaluación para que las clases invisibles tengan más oportunidad sobre las vistas, pero esto agrega información extra que en situaciones reales no tenemos.\\

La \autoref{tab:resultados-gzsd} muestra los resultados para GZSD evaluados en COCO. Se puede observa que el despeño promedio de las clases vistas e invisible para las métricas \textit{100@Recall} y \textit{mAP@0.5}, fue 3.84 puntos y 0.13 respectivamente. Como es de esperarse se obtuvo un mejor rendimiento para las clases vistas en ambas métricas.

Si comparamos los resultados de ZSD vs GZSD, se observa una disminución en los valores de las métricas promedio, que también se ve reflejado en los trabajos \cite{bansal2018zero} y \cite{rahman2020zero}. El motivo de esto es que las clases vistas, al estar en entrenamiento, tienden a tener un mejor puntaje en la etapa de evaluación que las clases invisibles pertenecientes a una misma clase superior. Por ejemplo, supongamos que la clase superior ``animal'' esta conformada por ``gato'' (clase vista) y ``perro'' (invisible), por la semejanza visual de estas dos clases, la proyección entrenada con ``gato'' va a tender a ubicar la proyección de una imagen de un ``perro'' cerca del vector semántico ``gato'', generando así un falso positivo, y causando una disminución en las métricas. Por este motivo, muchos objetos que en la configuración anterior se predecían correctamente, ahora se predicen incorrectamente por que una clase visible obtiene mejor puntaje.\\


\begin{table}[]
	\centering
	\resizebox{\textwidth}{!} {
		\begin{tabular}{|l|c|c|c|}
			\hline
			\multicolumn{1}{|c|}{\multirow{3}{*}{Modelo}} & \multicolumn{3}{c|}{GZSD}                                                       \\ \cline{2-4} 
			\multicolumn{1}{|c|}{}                        & Clases vistas             & Clases Invisibles        & Media                    \\ \cline{2-4} 
			\multicolumn{1}{|c|}{}                        & mAP/Recall Bansal/Recall  & mAP/Recall Bansal/Recall & mAP/Recall Bansal/Recall \\ \hline
			Mejor resultado \cite{bansal2018zero}                                        & -/15.02/-                 & -/15.32/-                & -/15.17/-                \\ \hline
			Nuestro modelo base                              & 0.15/\textbf{20.98}/4.77           & 0.11/\textbf{18.53}/2.92          & 0.13/\textbf{19.75}/3.84           \\ \hline
			Mejor resultado de \cite{rahman2020zero}     & \textbf{13.93}/-/\textbf{20.42}             & \textbf{2.55}/-/\textbf{12.42}             & \textbf{4.31}/-/\textbf{15.45}             \\ \hline
		\end{tabular}
	}
	\caption{Resultados obtenidos, en el desafió GZSD, para los modelos de Bansal \etal~\cite{bansal2018zero}, nuestro (ResNet) y Rahman \etal~\cite{rahman2020zero}}
	\label{tab:resultados-gzsd}
\end{table}
\newpage

\section{Resultados cualitativos} \label{sec:resultadoscualitativos}

Para tener una idea más realista del comportamiento del modelo propuesto, se realizaron pruebas sobre algunas imágenes de muestra. Con el objetivo de obtener ejemplos más claros fue necesario modificar la metodología que se utilizó para calcular los resultados cuantitativos. De esta manera, i) disminuimos el número de propuestas al orden de 10, y ii) se descartaron todas aquellas propuestas que obtuvieran un puntaje de confianza menor a 0.5. Esto hace que los ejemplos solo contengan los objetos más relevantes pero, en contra parte, se ignoran muchos objetos que se encuentran en segundo plano. Otra aclaración importante es que se evalúa ZSD y no GZSD, es por esto que se ignoran o confunden clases visibles. La \autoref{fig:ejmeplosModelo} muestra las detecciones del modelo propuesto en el conjunto de datos COCO. Los cuadros en azul muestran detecciones incorrectas y en verde las que acertaron a que clase pertenece el objeto.\\

Si bien el modelo confunde algunas instancias de objetos, cabe destacar que por lo general se equivoca dentro de una misma clase superior, confirmado que el modelo propuesto es capaz de relacionar aspectos visuales y detectar clases invisibles sin observar ninguna muestra durante el entrenamiento. 

Otro punto es que debido a que reducimos la cantidad de propuestas, éstas no están centradas en los objetos, y no detectan otros que se encuentran en un primer plano. Además, con esta configuración resulta difícil encontrar un ejemplo en el que se observen objetos pequeños, como  un ``cuchillo''; ya que las propuestas son a escalas más grandes y se ven opacadas por los objetos que lo rodean, como un ``plato''.

\begin{figure}[]
	\includegraphics[width=1\textwidth]{dibujo.png}
	\caption{Ejemplo del comportamiento del modelo sobre clases invisibles. Los cuadros azul se muestran las predicciones incorrectas y en verde las correctas.}
	\label{fig:ejmeplosModelo}
\end{figure}