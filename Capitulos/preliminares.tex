\chapter{Marco teórico}\label{cap:marcoteorico}

Para poder entender como funcionan los modelos de ZSD, primero, es necesario comprender como se componen. Por lo general utilizan multimodales, es decir, combinan los espacios semánticos y visuales para poder detectar objetos no vistos. A su ves para poder extraer caracteristicas visuales se necesita utilizar redes neuronales convolucionales y un algoritmo capas de indicar donde hay objetos en una imagen, por otro lado, se requiere un mecanismo para generar las caracteristicas semanticas como por ejemplo Word embedding.

En este capitulo se exponen cada componete que utilizan los modelos de ZSD, como estos se emplean y relacionan.

\section {Zero-shot learning (ZSL)} \label{sec:zeroshotlearning}
ZSL es un conjunto de problemas de aprendizaje automático, donde en el momento de la prueba, se observan muestras de clases que no se observaron durante el entrenamiento y se necesita predecir la categoría a la que pertenecen. Esta se diferencia de las configuraciones estándares en el aprendizaje automático, donde se espera que se clasifiquen correctamente las nuevas muestras en las clases que ya se han observado durante el entrenamiento.
Podemos diferenciar dos tipos de clases, las vistas que están presente en el entrenamiento y las invisibles o novedosas que no estuvieron en el mismo. Se debe proporcionar algún tipo de información complementaria sobre estas clases invisibles, este tipo de dato puede ser variado. Como por ejemplo, una descripción estructurada predefinida, que al tenerla en cuenta mejora el aprendizaje, o una descripción textual, donde las clases van acompañadas de un comentario en lenguaje natural, etc. Por último, tanto las clases visibles como las invisibles están relacionadas en un espacio vectorial, donde el conocimiento de las clases vistas se puede transferir a clases invisibles.\\

Para poder entender mejor como funciona ZSL, supongamos que queremos detectar un animal en extinción para poder rastrearlos y tener una forma de contabilizarlos, por ejemplo el yaguareté que habita el Chaco Argentino. Poder conseguir un conjunto considerable de fotos del mismo en distintos entornos, situaciones y ubicaciones, como de día, noche, corriendo, descansando, etc. puede resultar un gran desafió y demasiado tarde si nos lleva mucho tiempo. Pero se puede proponer un modelo de ZSL, entrenado con  imágenes de animales similares como leopardo, puma, tigre, etc. y reforzar el entrenamiento con algunas imagenes de yaguareté. \textcolor{red}{/* ToDo: Como formalizar el ejemplo?*/}\\

El problema de zero-shot learning se puede dividir en categorías según los datos presentes durante la fase de entrenamiento y la fase de prueba:
\begin{itemize}
	\item En base a los datos disponibles en el momento de entrenar un modelo.
	\begin{itemize}
		\item \textbf{Zero-shot learning inductivo:} Se tiene acceso a los datos y a la información complementario de solo las clases vistas.
		\item \textbf{Zero-shot learning transductivo:} Además de los datos y la información complementario de las clases vistas,  se tiene acceso a los datos de las clases no vistas.
	\end{itemize}
	\item Basado en los datos disponibles en el momento de la inferencia.
	\begin{itemize}
		\item \textbf{Zero-shot learning convencional (ZSL):} En las pruebas solo se evalúan las clases no vistas.
		\item \textbf{Zero-shot learning generalizado (GZSL):} En las pruebas se evalúan tanto las clases vista como las no vistas.
	\end{itemize}
\end{itemize}

\section {Zero-shot object detection (ZSD)} \label{sec:zeroshotobjectdetection}
Zero-shot object detection, tiene como objetivo reconocer y localizar simultáneamente instancias de objetos que pertenecen a categorías novedosas sin ningún ejemplo de entrenamiento. Como estas categorías no están presente en el entrenamiento, no se tiene ninguna información sobre su aspecto visual, lo cual no nos permite detectarlas ni reconocerlas. Por lo tanto, es necesario encontrar algún dominio que tenga la capacidad de guardar la información de todas las clases, para luego relacionarlas con el aspecto visual de las categorías novedosas.\\

Este trabajo se basa en el artículo científico de Bansal \etal ~\cite{bansal2018zero}, además utiliza muchos conceptos sobre zero-shot learning generalizado~\cite{zero-shot-generalizado}. Se propone un modelo zero-shot  inductivo, es decir, solo se observan imágenes de clases vistas y etiquetas que indican a que clase pertenece. Estas etiquetas son palabras del lenguaje natural sin ninguna estructura. Luego, se puede inferir todas las clases o solo las invisibles, dependiendo de si se quiere evaluar aprendizaje por zero-shot  generalizado o convencional, respectivamente.\\ 

El requisito estricto de no utilizar ninguna imagen de clase invisible durante el entrenamiento es una condición difícil. Además, existen otras dificultades en la tarea de ZSD relacionadas al conjunto de datos de entrenamiento y prueba, es decir entre las clases vistas e invisibles. Estas dificultades son:

\begin{itemize}
	\item \textbf{Rareza}: los conjuntos de datos, por lo general, contiene un problema de distribución, es decir, muchas clases raras tienen menos cantidad de instancias. Este problema hace que las clases con mayor cantidad de instancias sesguen el modelo y las clases más raras sean marcadas incorrectamente en la etapa de prueba. Esto es un problema al momento de comparar dos modelos que fueron entrenados con distintas clases, ya que algunas separaciones  de las clases resultan mejores que otras.
	
	\item \textbf{Tamaño del objeto}: algunas clases de objetos raros (tijeras, lápices, celulares, etc.), suelen tener un tamaño pequeño. Los objetos más pequeños son difíciles de detectar y reconocer. También, tienen el problema de que por lo general están junto a objetos más grandes como una mesa o una persona y se ven opacadas por estas clases.
	
	\item \textbf{Diversidad}: cuando una clase invisible no tiene otras clases visualmente similares, resulta muy difícil aprender el aspecto visual de esta. Por ejemplo, ``auto'' tiene muchas clases similares en comparación con ``cartel''. Esto permite una descripción visual inadecuada de la clase invisible ``cartel'' que eventualmente afectará el rendimiento de ZSD, a diferencia de lo que sucede con la clase ``auto''.
	
	\item \textbf{Ruido en el espacio semántico}: cuando se utiliza los vectores de incrustación semántica no supervisados como Word2Vec~\cite{mikolov2013distributed} o GloVe~\cite{pennington2014glove}, las embeddings resultante en general son ruidosas, ya que se generan automáticamente a partir de la minería de texto no anotado. Esto también afecta significativamente el rendimiento de ZSD.
\end{itemize}

Existen algunas variaciones de ZSD que intentan atenuar estos problemas, agrupando las clases en grupos o metaclases. Estas variaciones son: 

\begin{itemize}
	
	\item \textbf{Zero-shot metaclass detection (ZSMD)}, que dada una imagen de prueba, el objetivo es localizar cada instancia de una clase de objeto invisible y categorizarla en una de las superclases.
	
	\item \textbf{Zero-shot tagging (ZST)}, consiste en reconocer una o más clases invisibles en una imagen de prueba, sin identificar su ubicación.
	
	\item \textbf{Zero-shot metaclass tagging (ZSMT)}, reconoce una o más metaclases en una imagen de prueba, sin identificar su ubicación.
	
\end{itemize}

Estas tareas son atractivas a la hora de calcular métricas, pero no para ser aplicadas a problemas reales.


\section{Redes neuronales convolucionales} \label{sec:redesneuronalesconvolucionales}
Las redes neuronales convolucionales, o CNN por sus siglas en inglés, son un tipo de modelos de aprendizaje profundo (\textit{Deep learning}) utilizadas  para procesar distintos tipos de datos, pero empleadas generalmente en el dominio de las imágenes. Está inspirado en la organización de la corteza visual de los animales, diseñada para aprender de forma automática y adaptativa patrones en jerarquías, de bajo a alto nivel. Esto quiere decir que las primeras capas pueden detectar lineas, curvas y se van especializando hasta llegar a capas más profundas que reconocen formas complejas como un rostro. Por lo genera una red CNN se compone de tres tipos de capas: convolución, agrupación y capas completamente conectadas, como se puede ver en la \autoref{fig:CNNEjemplo}. El rol de cada capa es:

\begin{itemize}
	\item Convolución: somete los datos de entrada a un conjunto de filtros convolucionales, cada uno de los cuales activa ciertas características de los datos. Generalmente esta acompañada de una capa de activación, que permite un entrenamiento más rápido y eficaz al asignar los valores negativos a cero y mantener los valores positivos, de esta manera permitir que solo las características activadas pasen a la siguiente capa.
	\item Agrupación: se coloca generalmente después de la capa convolucional. Su utilidad principal radica en la reducción de su entrada para la siguiente capa convolucional, reduciendo así el número de parámetros que la red necesita aprender.
	\item Capas completamente conectadas: Es la encargada de relacionar los datos de las capas anteriores y generar una salida, que por lo general es utilizada para clasificar los datos de entrada.
\end{itemize}


Algunos ejemplos de redes CNN son: VGG16~\cite{simonyan2014very} (que posee 13 capas de convolución, 5 de agrupación y una totalmente conectada) y AlexNet~\cite{krizhevsky2012imagenet} (que contiene 5 capas convolucionales, 3 capas de agrupación y 3 capas completamente conectadas).\\

Estas redes son fundamentales para un modelo de detección de objeto y por lo tan tanto para ZSD. Debido a que las CNN son muy eficaces reconociendo patrones, si dos imágenes tienen un aspecto similar, los vectores también tendrán una semejanza. Esta es la forma que tenemos en este trabajo de relacionar los aspectos visuales de las clases vista e invisibles. Por ejemplo, supongamos un lavarropa y un lavavajilla que tienen un aspecto similar, donde la primera es una clase visible y la segunda es invisible, gracias a que las CNN generan vectores muy parecidos se puede obtener la información de que estas clases son similares sin a ver visto antes un lavavajilla. Por lo general se elimina la capa totalmente conectada y se utiliza la salida de la etapa previa como un vector de las caracteristicas visuales.

\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{img/red_cnn.png}
	\caption{Una arquitectura simplificada de una red neuronal convolucional.}
	\label{fig:CNNEjemplo}
\end{figure}

\section{Word embedding} \label{sec:wordembedding}
Así como en las imágenes utilizamos las redes CNN, para obtener un vector que represente a la misma, es necesario un procedimiento para representar palabras con algún objeto matemático. Hay muchas formas de representar palabras, la más usada son los \textit{word embedding}. Esta es una técnica de aprendizaje en el campo de procesamiento del lenguaje natural (PLN), capaz de capturar el contexto de una palabra en un documento, calcular similitud semántica y sintáctica con otras palabras.\\

Para entender como funcionan, consideremos las oraciones con un significado similar: ``Que tengas un buen día.'' y ``Que tengas un gran día.''. Si construimos un vocabulario exhaustivo:
 \[ V = \{que, tengas, un, buen, gran, dia\}. \]
A partir de esto, se puede crear un vector codificado para cada una de estas palabras, en donde cada vector tenga el tamaño de $V$, cuyos componentes sean todos 0 excepto por el elemento en el índice que representa la palabra correspondiente en el vocabulario, que contiene un 1. Esta representación no resulta conveniente ya que la distancia entre \textit{gran} y \textit{buen} es la misma que entre \textit{tengas} y \textit{buen}.  El objetivo es que las palabras con un contexto similar ocupen posiciones espaciales cercanas. Para lograr esto, se introduce cierta dependencia de una palabra con las otras.\\

Word2Vec~\cite{mikolov2013distributed} desarrollado por Tomas Mikolov en 2013. Es un modelo particularmente eficiente desde el punto de vista computacional. Este modelo se encuentra disponible de dos formas: \textit{Continuous Bag-of-Words} (CBOW) o el modelo \textit{Skip-Gram}. En CBOW, las representaciones distribuidas de contexto (o palabras circundantes) se combinan para predecir la palabra en el medio. En nuestro ejemplo \textit{gran} y \textit{buen} están rodeado de un contexto similar por lo cual resultan en vectores similares. Es varias veces más rápido de entrenar que el \textit{Skip-gram}, y tiene una precisión ligeramente mejor para las palabras frecuentes. Mientras que en el modelo \textit{Skip-gram}, la representación distribuida de la palabra de entrada se usa para predecir el contexto. Se entrena con una tarea falsa que, dada una palabra, intenta predecir las palabras vecinas. En realidad, el objetivo es solo aprender los pesos de la capa oculta que corresponden a los vectores de palabras que estamos tratando de aprender. Por ejemplo, \textit{Gran} se entrena para predecir el contexto \textit{un} y  \textit{día}, al igual que \textit{buen}. Funciona bien con una pequeña cantidad de datos de entrenamiento.

En este trabajo, aprovechamos la capacidad de capturar similitudes semántica que tiene \textit{word embedding}, para relacionar las clases vistas con las clases invisibles.\\


\section{Propuesta de objetos} \label{sec:propuestadeobjetos}
En problemas de detección de objetos, generalmente se tiene que encontrar todos los objetos posibles en la imagen. La localización de objetos se refiere a identificar la ubicación de uno o varios objetos en la imagen. Un algoritmo de localización de objetos generará las coordenadas de la ubicación de los objetos con respecto a la imagen. En visión artificial, la forma más popular de representar la ubicación de los objetos es con la ayuda de cuadros delimitadores (\textit{Bounding Boxes}). Existen muchos algoritmos y redes que intenta resolver este problema, algunos ejemplos son ventana deslizante (\textit{slide window}), Edge-Boxes~\cite{zitnick2014edge} y búsqueda selectiva (\textit{selective search})~\cite{uijlings2013selective}. En ZSD la propuesta de objetos cumple un papel importante, ya que se necesita extraer todas las instancias de los objetos, pero también tiene que discriminar fondos como cielo, ciudades, veredas, etc.\\

En este proyecto, como veremos en el \autoref{cap:experimentos} se experimentó con Edge-Boxes y \textit{selective search}, ya que estas generan una cantidad de propuestas significativamente menor a algoritmos del estilo de ventana deslizante. Aun así, procesar todas estas propuestas es engorroso. Adamas, estos modelos por lo general dan como resultados muchos cuadros con una gran superposición. Esto da lugar a una técnica denominada supresión de no máximos (NMS), ejemplificada en la~\autoref{fig:NMS}. Este algoritmo necesita de un puntaje que indica la confianza del cuadro delimitador y un criterio para comparar entre distintos cuadros. El criterio más común es Intersección sobre Unión (IoU), en la~\autoref{fig:IoU} se muestra como se calcula sobre dos \textit{Bounding Boxes}. La salida de NMS es un conjunto más reducido de propuestas, en la cual se filtraron todas las que se consideran repetidas y retorna solo las más representativa. A continuación se muestra el pseudocódigo de NMS.

\begin{center}
\noindent\fbox{
	\begin{minipage}{1\textwidth}
		\begin{algorithmic}[1]
			\Procedure{NMS}{B, S, t}
				\State{D = $\emptyset$ }
				\For {B $\neq \emptyset$ }
					\State{$b_i$ = \textbf{SelPropuestaMaxPuntaje}(B, S)}
					\State{\textbf{Eliminar}($b_i$, B)}
					\For {$b_j$ $\in$ B}
						\If {\textbf{IoU}($b_i$, $b_j$) $>$ t}
							\State{\textbf{Eliminar}($b_j$, B)}
						\EndIf
					\EndFor				
				\EndFor
			\State{\Return {D}}
			\EndProcedure
		\end{algorithmic}
	\end{minipage}
}
\end{center}

\begin{figure}[]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{img/iou.png}
		\caption{IoU}
		\label{fig:IoU}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1.1\textwidth]{img/NMS.png}
		\caption{Supresión de no máximos}
		\label{fig:NMS}
	\end{subfigure}
	\caption{(a) Calculo de Intersección sobre Unión. (b) Salida de la propuesta de objetos y el resultado después de NMS.}
		\label{fig:RP}
\end{figure}

\section{Multimodales} \label{sec:multimodales}

Nuestra experiencia del mundo es multimodal, vemos objetos y sus entornos, escuchamos música y ruidos, sentimos la textura de los distintos materiales, etc.  Inconscientemente asociamos una situación a los distintos estímulos que recibimos en ese momento y los relacionamos entre si, para genera una idea de lo que esta sucediendo. De esta manera, sabemos que si algo huele mal, lo más probable es que sepa igual, o podemos relacionar una imagen del campo con el sonido de los pájaros.\\

La modalidad se refiere a la forma en que algo sucede o se experimenta. Un problema de investigación se caracteriza como multimodal cuando incluye datos de distinta naturaleza. Para que la inteligencia artificial avance en la comprensión del mundo que nos rodea, necesita poder interpretar y relacionar estas distintas señales. Si bien la combinación de diferentes modalidades para mejorar el rendimiento parece una tarea ``atractiva'', en la práctica, es un desafío, ya que una ineducada combinación de distintos tipos de información, puede generar confusión y conflictos. 

La idea general es, a partir de dos objetos matemáticos distintos, uno de cada modal, poder transformarlos para lograr que pertenezcan a un tercer objeto que corresponde a la representación multimodal. Las imágenes suelen estar asociadas con etiquetas y explicaciones de texto. En este trabajo nos aprovechamos de esto y tratamos de encontrar un espacio común entre el vector que representan a la imagen del objeto y el que representa la sintaxis de la clase.

La idea de multimodales es utilizar un espacio compartido entre las representaciones de visión y del lenguaje. Para lograr esto, se utiliza  \textbf{Words embeddings} y \textbf{vectores con representaciones visuales}. Las primeras asignan a palabras una representación vectorial continua. Estos vectores se utilizan para medir similitudes semánticas y sintácticas entre palabras. Entre los modelos más famosos se encuentran Glove~\cite{pennington-etal-2014-glove} y Word2vec~\cite{mikolov2013efficient}. Por otro lado, para obtener los vectores visuales de una imagen se utilizan redes profundas. Entre los mejores modelos se encuentran VGG~\cite{simonyan2014very}, ResNet~\cite{resnet} e Inception~\cite{Szegedy_2015_CVPR}. La~\autoref{fig:EjemploZSD} describe como se utiliza la combinación de vectores de palabras y visuales para inferir un objetos nunca antes vistos por el modelo.\\

\begin{figure}[]
	\centering
	\includegraphics[width=0.7\textwidth]{img/Modelo.png}
	\caption{Descripción de la tarea de detección de objetos por disparo cero utilizando multimodales, donde los objetos ``Auto'', ``Camión'', ``Perro'' y ``Tigre'' se observan  durante el entrenamiento,  ``Gato'' y ``Camioneta'' son clases invisibles. El enfoque localiza estas clases no vistas aprovechando las relaciones del espacio semántico.}
	\label{fig:EjemploZSD}
\end{figure}
