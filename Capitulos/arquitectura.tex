\chapter{Diseño y Arquitectura}\label{cap:arquitectura}

\section{Arquitectura}
\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{img/arquitectura.png}
	\caption{Arquitectura propuesta y la dimensión de cada paso.}
	\label{fig:arqutectura}
\end{figure}

Como ya mencionamos anteriormente decidimos basarnos en el modelo propuesto por Ankan Bansal \cite{bansal2018zero}. Su arquitectura propuesta se puede dividir en las siguientes etapas.
\begin{itemize}
	\item \textbf{Pre-procesamiento:} Por cada imagen de entrenamiento, se extrae por cada cuadro delimitador una característica profunda utilizando una red neuronal convolucional y se asocia con el vector semantico a la clase que corresponde dicho cuadro, que se puede obtener de modelos de incrustación de palabras previamente entrenados como Glove o Word2vec. Esta etapa nos genera como salida dos listas $X = [\phi(b_0),...,\phi(b_k) \mid \phi(b_i) \in \mathbb{R}^{D_1}]$ $W = [w_0,...,w_k \mid w_i \in \mathbb{R}^{D_2}]$
	\item \textbf{Entrenamiento:} Utilizamos el espacio de incrustación común (${R}^{D_2}$) para calcular una medida de similitud entre las proyecciones  $\phi(b_i)$ y las words embedding $w_i$. Luego se entrena la proyección usando una pérdida de margen máximo que impone la restricción de que el puntaje de similitud de un cuadro delimitador con su clase verdadera debe ser más alto que el de otras clases. Para esto se utiliza una función de perdida definida como: \[\mathcal{L}(\psi_i, w_i) = \sum_{j \in \mathcal{S}, j\neq i} max(0, m - S_{ii} + S_{ij})\] $m$ es el margen máximo, y $S_{ij}$ es la similitud entre la proyección $i$-$esima$ y la incrustación $j$-$esima$. En la Figura \ref{fig:arqutectura}, se puede apreciar la arquitectura completa.
	 
	 También se agrega una función de pérdida de reconstrucción como sugiere Kodirov en \cite{kodirov2017semantic}. Se utilizan las características del cuadro delimitado proyectada para reconstruir las características profundas originales y calcular la pérdida de reconstrucción como la distancia $L2$  entre la característica reconstruida y la característica profunda original. \[\mathcal{L}_r = \Vert{\phi(b_i) - \psi_iW_p^T}\Vert^2 \] Luego definimos $\lambda$ como un coeficiente de ponderación que controla la importancia del primer y segundo término, que corresponden a las pérdidas de proyección y reconstrucción, respectivamente. Por lo cual la función de perdida total es: \[\mathcal{L}_t = \lambda \mathcal{L} + (1-\lambda) \mathcal{L}_r \]
	\item \textbf{Evaluación:} Por cada imagen del conjunto de entrenamiento, se genera un conjunto de propuestas de cuadros delimitadores. Luego, se eliminan todos lo que no tienen un puntaje de confianza mayor a un umbral. Para cada cuadro se computa la característica profunda $\phi(b_i)$ y utilizando la matriz $W_p$, para predecir las característica semántica. Por ultimo, se calcula la similitud con todas las características semántica, asignando al nuevo cuadro delimitador la que tenga mayor puntaje.
\end{itemize}
Es común que en la detección de objetos incluyan una clase de fondo para aprender un detector robusto que pueda discriminar eficazmente entre objetos de primer plano y objetos de fondo. En ZSD, esto no es un problema trivial, ya que no sabemos si un cuadro de fondo incluye elementos de fondo como cielo, tierra, bosque, etc. o una instancia de una clase de objeto invisible. En muchos trabajamos se proponen distintas técnicas para abordar este problema, pero no presentan mejoras en evaluaciones cuantitativas. Es por esto que no se incluye una arquitectura que discrimine cuadros de fondos.

Las propuestas de cuadro delimitadores son claves a la hora de evaluar un modelo, como mencionamos en secciones anteriores.  Muchas arquitecturas incluyen una red de propuestas regionales, RPN por sus siglas en ingles. De esta manera entrenan un modelo completo que también aprende a generar propuestas. Por lo que se pudo investigar, en nuestro caso resulta mas conveniente utilizar un modelo pre-entrenado como Edge-Boxes o Selective search.

\section{Conjuntos de datos}
COCO es un conjunto de datos de detección, segmentación y subtítulos de objetos a gran escala. COCO tiene varias características: Segmentación de objetos,  Reconocimiento en contexto,  Segmentación de material de superpíxeles, 330 mil imágenes ($>$ 200 mil etiquetadas), 1,5 millones de instancias de objetos y 80 categorías de objetos.

 La gran cantidad de instancias de objetos y de categorías, resulta en un conjunto ideal para entrenar y evaluar modelos de ZSD. Ademas la mayoría de la imágenes consta de una gran cantidad de objetos que generan un contexto y no de uno solo centralizado, como son por ejemplo las imágenes del conjunto Visual Genome Dataset. Utilizamos las imágenes de entrenamiento del conjunto COCO 2014 e imágenes del conjunto de validación para realizar pruebas.
\\
Como COCO no provee una separación de los datos para evaluar modelos de ZSD, es necesario crear una forma de dividirlos. Notar que resulta de suma importancia dividir las clases, de tal manera que para todo objeto del conjunto prueba se encuentre otro similar en entrenamiento. Ademas, no se puede encontrar ningún objeto de prueba en los datos de entrenamiento. Dicho esto, se propuso una manera de separar las imágenes. COCO también tiene agrupada las clases por ``Clases superiores''
\begin{figure}[H]
	\begin{center}
	\centering
	\includegraphics[width=0.9\textwidth]{img/data_set.png}
	\caption{Divisional de las clases para entrenamiento (verde) y pruebas (azul).}
	\label{fig:data_set}
	\end{center}	
\end{figure}
Por cada ``Clase superior'', elegimos de forma aleatoria un 70\% de clases para entrenamiento y un 30\% para pruebas. Es decir 47 y 18 clases respectivamente. En la Figura \ref{fig:data_set} se puede ver como quedan divididas. Por ultimo se eliminaron todas las imágenes de entrenamiento que contengan al menos una instancia de las clases de prueba. Esto resulta en 42564 imágenes con 261258 instancias de entrenamiento y 3008 con 10878 instancias de prueba. Bansal \cite{bansal2018zero}, divide el conjunto de datos, de manera similar, utiliza la misma cantidad de clases para pruebas y entrenamiento. Pero para crear las metaclases utiliza las incrustaciones de vectores de palabras para todas las clases y las agrupa en grupos de 10 usando la similitud de coseno entre los vectores de palabras como métrica. Por los problemas mencionados en la \autoref{cap:ZSD}, también utilizamos esta separación, de esta manera logramos una comparación de modelos mas justa.

\begin{figure}[H]
	\begin{center}
	\begin{subfigure}{.3\textwidth}
		\includegraphics[width=0.85\textwidth]{img/cifar-zsd-test400.jpg}
		\label{fig:ex1}
	\end{subfigure}
	\begin{subfigure}{.3\textwidth}
		\includegraphics[width=0.85\textwidth]{img/cifar-zsd-test379.jpg}
		\label{fig:ex2}
	\end{subfigure}
	\begin{subfigure}{.3\textwidth}
		\includegraphics[width=0.85\textwidth]{img/cifar-zsd-test283.jpg}
		\label{fig:ex3}
	\end{subfigure}
	\caption{Ejemplos de imágenes del conjunto de datos CIFAR-ZSD.}
	\label{fig:CIFAR-ZSD}
	\end{center}
\end{figure}

COCO puede resultar pesado lo cual implica mucho tiempo de cómputos, para soluciona esto se creo un conjunto de datos sintéticos basado en CIFAR-100 datasets, el cual denominamos CIFAR-ZSD. Consta de imágenes localizadas, rotadas y re-escalada aleatoriamente con un fondo de otra imagen. Con esto se intenta simular imágenes reales en la cual un objeto puede aparecer con distintos aspectos y escalas. Este conjunto esta divido para que ninguna instancia de prueba  aparezca en el conjunto de entrenamiento.

Aunque resulta muy útil para probar modelos, no es bueno para reportar métricas reales. Pero en combinación con COCO, que si lo es, ambos cooperan para enfrentar el problema de ZSD de una forma mas practica.

\section{Detalles de la implementación}

El paper de Bansal \cite{bansal2018zero}, carece de una implementación de acceso publico, por lo cual se realizo una implementación propria, basándose en los detalles que se pueden extraer del documento publicado. Fue necesario algunas presunciones, pero también, nos otorgo flexibilidades a la hora de codificar. Se busco obtener los resultados mas cerca a los reportados, pero siendo lo mas fiel a la información disponible. Para esto se decidió utilizar Python 3 con el framework Keras ejecutadose sobre TensorFlow. Si se quiere acceder a mas detalles, el código se encuentra en \url{https://github.com/agustinhurquiza/Tesis}.\\

Primero se realiza un preprocesamiento a todas las imagines del conjunto de datos de entrenamiento, que consiste en generar propuestas de objetos utilizando \textbf{Edge Boxes} o \textbf{Selective Search}. En el \autoref{cap:experimentos} se muestra los resultados de usar cada algoritmo. Luego por cada propuesta se calcula la intersección sobre unión con todos los cuadros delimitadores verdaderos. Si el IoU $> 0.5$ con algún cuadro verdadero, se guarda la propuesta y se la asocia con la clase del cuadro delimitador verdadero. Este preprocesamiento aumenta significativamente la cantidad de instancias para la etapa de entrenamiento. Resulta en un total de 1.436.835 instancias para COCO y 137.204 para CIFAR-ZSD. El siguiente paso consiste en generar el vector de características visuales y semánticas de cada cuadro delimitador. Como las CNN, utilizan un tamaño de entrada fijo, es necesario rescalar todos los cuadros.  Para \textbf{VGG16} utilizamos $224 \times 224$  y $299 \times 299$ en \textbf{Inception ResNet V2}, que son las dimensiones por defecto. El tamaño del vector de salida de cada red es de 512 en VGG y 1536 ResNet. Para ambas usamos pesos preentrenado en \textbf{Imagenet}. Este paso es diferente a como se hace en el paper de Bansal \cite{bansal2018zero}, ya que en este trabajo el modelo se entrena de extremo a extremo, es decir los pesos de la CNN se ajustan en la etapa de entrenamiento. Para obtener las características semánticas, utilizamos \textbf{Word2Vec} previamente entrenado de Google. Que Incluye vectores para un vocabulario de 3 millones de palabras y frases que se entreno en aproximadamente 100 mil millones de palabras de un conjunto de datos de Google News. La longitud del vector resultante es de 300. Luego se guardan dos archivos por cada imagen, en uno se encuentran los vectores visuales de cada clase y en el otro los semánticos.\\

Por ultimo, para el entrenamiento, se creo una red con una sola capa oculta, una capa de entrada del tamaño del vector de características visuales y una de salida de la dimensión de las características semánticas. Lo cual resulta en 153.900 parámetros entrenables con \textbf{VGG16} y 461.100 con \textbf{Inception ResNet V2}. Como optimizador se utilizo Adam, sin ninguna activación, un taza de aprendizaje de 10e-3 y un tamaño del lote de 64 muestras. Para la función de perdida se uso un lambda de 10e-3 y un margen máximo de 1.\\



